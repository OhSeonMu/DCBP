commit 79237124a96e35836d16b4c079c3818af9cefd22
Author: minlno <mhkim@dgist.ac.kr>
Date:   Tue May 14 15:27:04 2024 +0900

    enable to use pebs in kernel space like userspace mmap

diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 0031f7b4d..4cbf64002 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1506,6 +1506,7 @@ extern int perf_swevent_get_recursion_context(void);
 extern void perf_swevent_put_recursion_context(int rctx);
 extern u64 perf_swevent_set_period(struct perf_event *event);
 extern void perf_event_enable(struct perf_event *event);
+extern int perf_event_stop(struct perf_event *event, int restart);
 extern void perf_event_disable(struct perf_event *event);
 extern void perf_event_disable_local(struct perf_event *event);
 extern void perf_event_disable_inatomic(struct perf_event *event);
@@ -1744,4 +1745,9 @@ static inline bool branch_sample_priv(const struct perf_event *event)
 	return event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_PRIV_SAVE;
 }
 #endif /* CONFIG_PERF_EVENTS */
+
+extern int mtat__perf_event_init(struct perf_event *event, unsigned long nr_pages);
+extern int mtat__perf_event_open(struct perf_event_attr *attr_ptr, pid_t pid,
+		int cpu, int group_fd, unsigned long flags);
+
 #endif /* _LINUX_PERF_EVENT_H */
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 82aee96b3..6fa532d76 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -3076,7 +3076,7 @@ static int __perf_event_stop(void *info)
 	return 0;
 }
 
-static int perf_event_stop(struct perf_event *event, int restart)
+int perf_event_stop(struct perf_event *event, int restart)
 {
 	struct stop_event_data sd = {
 		.event		= event,
@@ -3102,6 +3102,7 @@ static int perf_event_stop(struct perf_event *event, int restart)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(perf_event_stop);
 
 /*
  * In order to contain the amount of racy and tricky in the address filter
@@ -12675,6 +12676,497 @@ SYSCALL_DEFINE5(perf_event_open,
 	return err;
 }
 
+/* allocates perf_buffer instead of calling perf_mmap() */
+int mtat__perf_event_init(struct perf_event *event, unsigned long nr_pages)
+{
+    struct perf_buffer *rb = NULL;
+    int ret = 0, flags = 0;
+
+    if (event->cpu == -1 && event->attr.inherit)
+	return -EINVAL;
+
+    ret = security_perf_event_read(event);
+    if (ret)
+	return ret;
+
+    if (nr_pages != 0 && !is_power_of_2(nr_pages))
+	return -EINVAL;
+
+    WARN_ON_ONCE(event->ctx->parent_ctx);
+    mutex_lock(&event->mmap_mutex);
+
+    WARN_ON(event->rb);
+
+    rb = rb_alloc(nr_pages,
+	    event->attr.watermark ? event->attr.wakeup_watermark : 0,
+	    event->cpu, flags);
+    if (!rb) {
+	ret = -ENOMEM;
+	goto unlock;
+    }
+
+    ring_buffer_attach(event, rb);
+    perf_event_init_userpage(event);
+    perf_event_update_userpage(event);
+
+unlock:
+    if (!ret) {
+	atomic_inc(&event->mmap_count);
+    }
+    mutex_unlock(&event->mmap_mutex);
+    return ret;
+}
+EXPORT_SYMBOL_GPL(mtat__perf_event_init);
+
+/* sys_perf_event_open for memtis use */
+int mtat__perf_event_open(struct perf_event_attr *attr_ptr, pid_t pid,
+	int cpu, int group_fd, unsigned long flags)
+{
+ 	struct perf_event *group_leader = NULL, *output_event = NULL;
+	struct perf_event *event, *sibling;
+	struct perf_event_attr attr;
+	struct perf_event_context *ctx, *gctx;
+	struct file *event_file = NULL;
+	struct fd group = {NULL, 0};
+	struct task_struct *task = NULL;
+	struct pmu *pmu;
+	int event_fd;
+	int move_group = 0;
+	int err;
+	int f_flags = O_RDWR;
+	int cgroup_fd = -1;
+
+	/* for future expandability... */
+	if (flags & ~PERF_FLAG_ALL)
+		return -EINVAL;
+
+	/* Do we allow access to perf_event_open(2) ? */
+	err = security_perf_event_open(&attr, PERF_SECURITY_OPEN);
+	if (err)
+		return err;
+
+	/*err = perf_copy_attr(attr_ptr, &attr);
+	if (err)
+		return err;*/
+	attr = *attr_ptr;
+
+	if (!attr.exclude_kernel) {
+		err = perf_allow_kernel(&attr);
+		if (err)
+			return err;
+	}
+
+	if (attr.namespaces) {
+		if (!perfmon_capable())
+			return -EACCES;
+	}
+
+	if (attr.freq) {
+		if (attr.sample_freq > sysctl_perf_event_sample_rate)
+			return -EINVAL;
+	} else {
+		if (attr.sample_period & (1ULL << 63))
+			return -EINVAL;
+	}
+
+	/* Only privileged users can get physical addresses */
+	if ((attr.sample_type & PERF_SAMPLE_PHYS_ADDR)) {
+		err = perf_allow_kernel(&attr);
+		if (err)
+			return err;
+	}
+
+	/* REGS_INTR can leak data, lockdown must prevent this */
+	if (attr.sample_type & PERF_SAMPLE_REGS_INTR) {
+		err = security_locked_down(LOCKDOWN_PERF);
+		if (err)
+			return err;
+	}
+
+	/*
+	 * In cgroup mode, the pid argument is used to pass the fd
+	 * opened to the cgroup directory in cgroupfs. The cpu argument
+	 * designates the cpu on which to monitor threads from that
+	 * cgroup.
+	 */
+	if ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))
+		return -EINVAL;
+
+	if (flags & PERF_FLAG_FD_CLOEXEC)
+		f_flags |= O_CLOEXEC;
+
+	event_fd = get_unused_fd_flags(f_flags);
+	if (event_fd < 0)
+		return event_fd;
+
+	if (group_fd != -1) {
+		err = perf_fget_light(group_fd, &group);
+		if (err)
+			goto err_fd;
+		group_leader = group.file->private_data;
+		if (flags & PERF_FLAG_FD_OUTPUT)
+			output_event = group_leader;
+		if (flags & PERF_FLAG_FD_NO_GROUP)
+			group_leader = NULL;
+	}
+
+	if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
+		task = find_lively_task_by_vpid(pid);
+		if (IS_ERR(task)) {
+			err = PTR_ERR(task);
+			goto err_group_fd;
+		}
+	}
+
+	if (task && group_leader &&
+		group_leader->attr.inherit != attr.inherit) {
+		err = -EINVAL;
+		goto err_task;
+	}
+
+	if (flags & PERF_FLAG_PID_CGROUP)
+		cgroup_fd = pid;
+
+	event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
+				 NULL, NULL, cgroup_fd);
+	if (IS_ERR(event)) {
+		err = PTR_ERR(event);
+		goto err_task;
+	}
+
+	if (is_sampling_event(event)) {
+		if (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {
+			err = -EOPNOTSUPP;
+			goto err_alloc;
+		}
+	}
+
+	/*
+	 * Special case software events and allow them to be part of
+	 * any hardware group.
+	 */
+	pmu = event->pmu;
+
+	if (attr.use_clockid) {
+		err = perf_event_set_clock(event, attr.clockid);
+		if (err)
+			goto err_alloc;
+	}
+
+	if (pmu->task_ctx_nr == perf_sw_context)
+		event->event_caps |= PERF_EV_CAP_SOFTWARE;
+
+	if (group_leader) {
+		if (is_software_event(event) &&
+		    !in_software_context(group_leader)) {
+			/*
+			 * If the event is a sw event, but the group_leader
+			 * is on hw context.
+			 *
+			 * Allow the addition of software events to hw
+			 * groups, this is safe because software events
+			 * never fail to schedule.
+			 */
+			pmu = group_leader->ctx->pmu;
+		} else if (!is_software_event(event) &&
+				is_software_event(group_leader) &&
+			   (group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
+			/*
+			 * In case the group is a pure software group, and we
+			 * try to add a hardware event, move the whole group to
+			 * the hardware context.
+			 */
+			move_group = 1;
+		}
+	}
+
+	/*
+	 * Get the target context (task or percpu):
+	 */
+	ctx = find_get_context(pmu, task, event);
+	if (IS_ERR(ctx)) {
+		err = PTR_ERR(ctx);
+		goto err_alloc;
+	}
+
+	/*
+	 * Look up the group leader (we will attach this event to it):
+	 */
+	if (group_leader) {
+		err = -EINVAL;
+
+		/*
+		 * Do not allow a recursive hierarchy (this new sibling
+		 * becoming part of another group-sibling):
+		 */
+		if (group_leader->group_leader != group_leader)
+			goto err_context;
+
+		/* All events in a group should have the same clock */
+		if (group_leader->clock != event->clock)
+			goto err_context;
+
+		/*
+		 * Make sure we're both events for the same CPU;
+		 * grouping events for different CPUs is broken; since
+		 * you can never concurrently schedule them anyhow.
+		 */
+		if (group_leader->cpu != event->cpu)
+			goto err_context;
+
+		/*
+		 * Make sure we're both on the same task, or both
+		 * per-CPU events.
+		 */
+		if (group_leader->ctx->task != ctx->task)
+			goto err_context;
+
+		/*
+		 * Do not allow to attach to a group in a different task
+		 * or CPU context. If we're moving SW events, we'll fix
+		 * this up later, so allow that.
+		 */
+		if (!move_group && group_leader->ctx != ctx)
+			goto err_context;
+
+		/*
+		 * Only a group leader can be exclusive or pinned
+		 */
+		if (attr.exclusive || attr.pinned)
+			goto err_context;
+	}
+
+	if (output_event) {
+		err = perf_event_set_output(event, output_event);
+		if (err)
+			goto err_context;
+	}
+
+	event_file = anon_inode_getfile("[perf_event]", &perf_fops, event,
+					f_flags);
+	if (IS_ERR(event_file)) {
+		err = PTR_ERR(event_file);
+		event_file = NULL;
+		goto err_context;
+	}
+
+	if (task) {
+		err = down_read_interruptible(&task->signal->exec_update_lock);
+		if (err)
+			goto err_file;
+
+		/*
+		 * We must hold exec_update_lock across this and any potential
+		 * perf_install_in_context() call for this new event to
+		 * serialize against exec() altering our credentials (and the
+		 * perf_event_exit_task() that could imply).
+		 */
+		err = -EACCES;
+		if (!perf_check_permission(&attr, task))
+			goto err_cred;
+	}
+
+	if (move_group) {
+		gctx = __perf_event_ctx_lock_double(group_leader, ctx);
+
+		if (gctx->task == TASK_TOMBSTONE) {
+			err = -ESRCH;
+			goto err_locked;
+		}
+
+		/*
+		 * Check if we raced against another sys_perf_event_open() call
+		 * moving the software group underneath us.
+		 */
+		if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
+			/*
+			 * If someone moved the group out from under us, check
+			 * if this new event wound up on the same ctx, if so
+			 * its the regular !move_group case, otherwise fail.
+			 */
+			if (gctx != ctx) {
+				err = -EINVAL;
+				goto err_locked;
+			} else {
+				perf_event_ctx_unlock(group_leader, gctx);
+				move_group = 0;
+			}
+		}
+
+		/*
+		 * Failure to create exclusive events returns -EBUSY.
+		 */
+		err = -EBUSY;
+		if (!exclusive_event_installable(group_leader, ctx))
+			goto err_locked;
+
+		for_each_sibling_event(sibling, group_leader) {
+			if (!exclusive_event_installable(sibling, ctx))
+				goto err_locked;
+		}
+	} else {
+		mutex_lock(&ctx->mutex);
+	}
+
+	if (ctx->task == TASK_TOMBSTONE) {
+		err = -ESRCH;
+		goto err_locked;
+	}
+
+	if (!perf_event_validate_size(event)) {
+		err = -E2BIG;
+		goto err_locked;
+	}
+
+	if (!task) {
+		/*
+		 * Check if the @cpu we're creating an event for is online.
+		 *
+		 * We use the perf_cpu_context::ctx::mutex to serialize against
+		 * the hotplug notifiers. See perf_event_{init,exit}_cpu().
+		 */
+		struct perf_cpu_context *cpuctx =
+			container_of(ctx, struct perf_cpu_context, ctx);
+
+		if (!cpuctx->online) {
+			err = -ENODEV;
+			goto err_locked;
+		}
+	}
+
+	if (perf_need_aux_event(event) && !perf_get_aux_event(event, group_leader)) {
+		err = -EINVAL;
+		goto err_locked;
+	}
+
+	/*
+	 * Must be under the same ctx::mutex as perf_install_in_context(),
+	 * because we need to serialize with concurrent event creation.
+	 */
+	if (!exclusive_event_installable(event, ctx)) {
+		err = -EBUSY;
+		goto err_locked;
+	}
+
+	WARN_ON_ONCE(ctx->parent_ctx);
+
+	/*
+	 * This is the point on no return; we cannot fail hereafter. This is
+	 * where we start modifying current state.
+	 */
+
+	if (move_group) {
+		/*
+		 * See perf_event_ctx_lock() for comments on the details
+		 * of swizzling perf_event::ctx.
+		 */
+		perf_remove_from_context(group_leader, 0);
+		put_ctx(gctx);
+
+		for_each_sibling_event(sibling, group_leader) {
+			perf_remove_from_context(sibling, 0);
+			put_ctx(gctx);
+		}
+
+		/*
+		 * Wait for everybody to stop referencing the events through
+		 * the old lists, before installing it on new lists.
+		 */
+		synchronize_rcu();
+
+		/*
+		 * Install the group siblings before the group leader.
+		 *
+		 * Because a group leader will try and install the entire group
+		 * (through the sibling list, which is still in-tact), we can
+		 * end up with siblings installed in the wrong context.
+		 *
+		 * By installing siblings first we NO-OP because they're not
+		 * reachable through the group lists.
+		 */
+		for_each_sibling_event(sibling, group_leader) {
+			perf_event__state_init(sibling);
+			perf_install_in_context(ctx, sibling, sibling->cpu);
+			get_ctx(ctx);
+		}
+
+		/*
+		 * Removing from the context ends up with disabled
+		 * event. What we want here is event in the initial
+		 * startup state, ready to be add into new context.
+		 */
+		perf_event__state_init(group_leader);
+		perf_install_in_context(ctx, group_leader, group_leader->cpu);
+		get_ctx(ctx);
+	}
+
+	/*
+	 * Precalculate sample_data sizes; do while holding ctx::mutex such
+	 * that we're serialized against further additions and before
+	 * perf_install_in_context() which is the point the event is active and
+	 * can use these values.
+	 */
+	perf_event__header_size(event);
+	perf_event__id_header_size(event);
+
+	event->owner = current;
+
+	perf_install_in_context(ctx, event, event->cpu);
+	perf_unpin_context(ctx);
+
+	if (move_group)
+		perf_event_ctx_unlock(group_leader, gctx);
+	mutex_unlock(&ctx->mutex);
+
+	if (task) {
+		up_read(&task->signal->exec_update_lock);
+		put_task_struct(task);
+	}
+
+	mutex_lock(&current->perf_event_mutex);
+	list_add_tail(&event->owner_entry, &current->perf_event_list);
+	mutex_unlock(&current->perf_event_mutex);
+
+	/*
+	 * Drop the reference on the group_event after placing the
+	 * new event on the sibling_list. This ensures destruction
+	 * of the group leader will find the pointer to itself in
+	 * perf_group_detach().
+	 */
+	fdput(group);
+	fd_install(event_fd, event_file);
+	return event_fd;
+
+err_locked:
+	if (move_group)
+		perf_event_ctx_unlock(group_leader, gctx);
+	mutex_unlock(&ctx->mutex);
+err_cred:
+	if (task)
+		up_read(&task->signal->exec_update_lock);
+err_file:
+	fput(event_file);
+err_context:
+	perf_unpin_context(ctx);
+	put_ctx(ctx);
+err_alloc:
+	/*
+	 * If event_file is set, the fput() above will have called ->release()
+	 * and that will take care of freeing the event.
+	 */
+	if (!event_file)
+		free_event(event);
+err_task:
+	if (task)
+		put_task_struct(task);
+err_group_fd:
+	fdput(group);
+err_fd:
+	put_unused_fd(event_fd);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mtat__perf_event_open);
+
 /**
  * perf_event_create_kernel_counter
  *

commit 5ee60a79e8fc046428a709e8e04eb54cfae716f0
Author: minlno <mhkim@dgist.ac.kr>
Date:   Wed Nov 29 18:48:14 2023 +0900

    update

diff --git a/Makefile b/Makefile
index 35fc0d628..09359001a 100644
--- a/Makefile
+++ b/Makefile
@@ -3,7 +3,7 @@ VERSION = 6
 PATCHLEVEL = 1
 SUBLEVEL = 53
 EXTRAVERSION =
-NAME = Curry Ramen
+NAME = Minho
 
 # *DOCUMENTATION*
 # To see a list of typical targets execute "make help"
diff --git a/make.sh b/make.sh
index 13d4c5120..066b69cb7 100755
--- a/make.sh
+++ b/make.sh
@@ -1,4 +1,4 @@
-NCPU=40
+NCPU=`nproc`
 make -j $NCPU
 make modules -j $NCPU
 sudo make INSTALL_MOD_STRIP=1 modules_install -j $NCPU

commit a93679d63a636b486af2b168f8a2010bf2ec7a8b
Author: minlno <mhkim@dgist.ac.kr>
Date:   Mon Oct 16 15:24:33 2023 +0900

    Export perf buffer related function

diff --git a/kernel/events/core.c b/kernel/events/core.c
index db1065daa..82aee96b3 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -6057,6 +6057,7 @@ void ring_buffer_put(struct perf_buffer *rb)
 
 	call_rcu(&rb->rcu_head, rb_free_rcu);
 }
+EXPORT_SYMBOL_GPL(ring_buffer_put);
 
 static void perf_mmap_open(struct vm_area_struct *vma)
 {
diff --git a/kernel/events/ring_buffer.c b/kernel/events/ring_buffer.c
index 273a0fe79..a829549fa 100644
--- a/kernel/events/ring_buffer.c
+++ b/kernel/events/ring_buffer.c
@@ -850,6 +850,7 @@ struct perf_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)
 fail:
 	return NULL;
 }
+EXPORT_SYMBOL_GPL(rb_alloc);
 
 void rb_free(struct perf_buffer *rb)
 {
@@ -940,6 +941,7 @@ struct perf_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)
 fail:
 	return NULL;
 }
+EXPORT_SYMBOL_GPL(rb_alloc);
 
 #endif
 

commit 1895713cd231a3c6d68f41f0d43125ac227d56c3
Author: minlno <mhkim@dgist.ac.kr>
Date:   Wed Sep 20 11:16:05 2023 +0900

    multi-threaded page copy

diff --git a/include/linux/highmem.h b/include/linux/highmem.h
index 44242268f..0711d6409 100644
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@ -347,6 +347,8 @@ static inline int copy_mc_user_highpage(struct page *to, struct page *from,
 
 #ifndef __HAVE_ARCH_COPY_HIGHPAGE
 
+int copy_page_multithread(struct page *to, struct page *from, int nr_pages);
+
 static inline void copy_highpage(struct page *to, struct page *from)
 {
 	char *vfrom, *vto;
diff --git a/mm/Makefile b/mm/Makefile
index 8e105e5b3..4709640cc 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -67,6 +67,7 @@ obj-y += page-alloc.o
 obj-y += init-mm.o
 obj-y += memblock.o
 obj-y += $(memory-hotplug-y)
+obj-y += copy_page.o
 
 ifdef CONFIG_MMU
 	obj-$(CONFIG_ADVISE_SYSCALLS)	+= madvise.o
diff --git a/mm/copy_page.c b/mm/copy_page.c
new file mode 100644
index 000000000..9b902bca8
--- /dev/null
+++ b/mm/copy_page.c
@@ -0,0 +1,113 @@
+/*
+ * Parallel page copy routine.
+ *
+ * Referenced code:
+ *   https://github.com/ysarch-lab/nimble_page_management_asplos_2019/tree/nimble_page_management_5_6_rc6
+ */
+
+#include <linux/highmem.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+unsigned int limit_mt_num = 4;
+
+/* ======================== multi-threaded copy page =========================== */
+
+struct copy_item {
+	char *to;
+	char *from;
+	unsigned long chunk_size;
+};
+
+struct copy_page_info {
+	struct work_struct copy_page_work;
+	unsigned long num_items;
+	struct copy_item item_list[0];
+};
+
+static void copy_page_routine(char *vto, char *vfrom,
+		unsigned long chunk_size)
+{
+	memcpy(vto, vfrom, chunk_size);
+}
+
+static void copy_page_work_queue_thread(struct work_struct *work)
+{
+	struct copy_page_info *my_work = (struct copy_page_info *) work;
+	int i;
+
+	for (i = 0; i < my_work->num_items; ++i)
+		copy_page_routine(my_work->item_list[i].to,
+				my_work->item_list[i].from, 
+				my_work->item_list[i].chunk_size);
+}
+
+int copy_page_multithread(struct page *to, struct page *from, int nr_pages)
+{
+	unsigned int total_mt_num = limit_mt_num;
+	int to_node = numa_node_id();
+	int i;
+	struct copy_page_info *work_items[40] = {0};
+	char *vto, *vfrom;
+	unsigned long chunk_size;
+	const struct cpumask *per_node_cpumask = cpumask_of_node(to_node);
+	int cpu_id_list[40] = {0};
+	int cpu;
+	int err = 0;
+
+	total_mt_num = min_t(unsigned int, total_mt_num,
+				cpumask_weight(per_node_cpumask));
+
+	if (total_mt_num > 1)
+		total_mt_num = (total_mt_num / 2) * 2;
+
+	for (cpu = 0; cpu < total_mt_num; ++cpu) {
+		work_items[cpu] = kzalloc(sizeof(struct copy_page_info)
+				+ sizeof(struct copy_item), GFP_KERNEL);
+
+		if (!work_items[cpu]) {
+			err = -ENOMEM;
+			goto free_work_items;
+		}
+	}
+
+	i = 0;
+	for_each_cpu(cpu, per_node_cpumask) {
+		if (i >= total_mt_num)
+			break;
+		cpu_id_list[i] = cpu;
+		++i;
+	}
+
+	vfrom = kmap(from);
+	vto = kmap(to);
+	chunk_size = PAGE_SIZE * nr_pages / total_mt_num;
+
+	for (i = 0; i < total_mt_num; ++i) {
+		INIT_WORK((struct work_struct *) work_items[i],
+				copy_page_work_queue_thread);
+
+		work_items[i]->num_items = 1;
+		work_items[i]->item_list[0].to = vto + i * chunk_size;
+		work_items[i]->item_list[0].from = vfrom + i * chunk_size;
+		work_items[i]->item_list[0].chunk_size = chunk_size;
+
+		queue_work_on(cpu_id_list[i],
+				system_highpri_wq,
+				(struct work_struct *) work_items[i]);
+	}
+
+	/* Wait until it finishes */
+	for (i = 0; i < total_mt_num; ++i)
+		flush_work((struct work_struct *) work_items[i]);
+
+	kunmap(to);
+	kunmap(from);
+
+free_work_items:
+	for (cpu = 0; cpu < total_mt_num; ++cpu)
+		if (work_items[cpu])
+			kfree(work_items[cpu]);
+
+	return err;
+}
diff --git a/mm/util.c b/mm/util.c
index ce3bb17c9..c7c21d154 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -23,6 +23,7 @@
 #include <linux/processor.h>
 #include <linux/sizes.h>
 #include <linux/compat.h>
+#include <linux/highmem.h>
 
 #include <linux/uaccess.h>
 
@@ -850,6 +851,10 @@ void folio_copy(struct folio *dst, struct folio *src)
 {
 	long i = 0;
 	long nr = folio_nr_pages(src);
+	int rc = copy_page_multithread(folio_page(dst, 0), folio_page(src, 0), (int)nr);
+
+	if (!rc)
+		return;
 
 	for (;;) {
 		copy_highpage(folio_page(dst, i), folio_page(src, i));

commit 74ee34c55c5d933a291fb20401e500e62b2ad102
Author: minlno <mhkim@dgist.ac.kr>
Date:   Tue Sep 19 18:44:44 2023 +0900

    implement enqueue/dequeue hook & export symbols

diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index 8eea709e3..3fe2fc982 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -1094,8 +1094,9 @@ static int hugetlbfs_migrate_folio(struct address_space *mapping,
 	int rc;
 
 	rc = migrate_huge_page_move_mapping(mapping, dst, src);
-	if (rc != MIGRATEPAGE_SUCCESS)
+	if (rc != MIGRATEPAGE_SUCCESS) {
 		return rc;
+	}
 
 	if (hugetlb_page_subpool(&src->page)) {
 		hugetlb_set_page_subpool(&dst->page,
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 58b53d08f..44a40eca1 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -1208,4 +1208,10 @@ bool want_pmd_share(struct vm_area_struct *vma, unsigned long addr);
 #define flush_hugetlb_tlb_range(vma, addr, end)	flush_tlb_range(vma, addr, end)
 #endif
 
+typedef struct page *(*dequeue_hook_t)(struct hstate *h, int nid);
+void set_dequeue_hook(dequeue_hook_t hook);
+
+typedef struct page *(*enqueue_hook_t)(struct hstate *h, struct page *page);
+void set_enqueue_hook(enqueue_hook_t hook);
+
 #endif /* _LINUX_HUGETLB_H */
diff --git a/include/linux/swap.h b/include/linux/swap.h
index a18cf4b7c..ef27f8c8b 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -417,6 +417,8 @@ extern void lru_cache_add_inactive_or_unevictable(struct page *page,
 						struct vm_area_struct *vma);
 
 /* linux/mm/vmscan.c */
+extern int mtat_folio_isolate_lru(struct folio *);
+extern void mtat_folio_putback_lru(struct folio *);
 extern unsigned long zone_reclaimable_pages(struct zone *zone);
 extern unsigned long try_to_free_pages(struct zonelist *zonelist, int order,
 					gfp_t gfp_mask, nodemask_t *mask);
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index c38ec6efe..7d50f1dd6 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -48,8 +48,11 @@
 #include "hugetlb_vmemmap.h"
 
 int hugetlb_max_hstate __read_mostly;
+EXPORT_SYMBOL(hugetlb_max_hstate);
 unsigned int default_hstate_idx;
+EXPORT_SYMBOL(default_hstate_idx);
 struct hstate hstates[HUGE_MAX_HSTATE];
+EXPORT_SYMBOL(hstates);
 
 #ifdef CONFIG_CMA
 static struct cma *hugetlb_cma[MAX_NUMNODES];
@@ -1275,10 +1278,22 @@ static bool vma_has_reserves(struct vm_area_struct *vma, long chg)
 	return false;
 }
 
+static enqueue_hook_t enqueue_hook = NULL;
+void set_enqueue_hook(enqueue_hook_t hook)
+{
+	enqueue_hook = hook;
+}
+EXPORT_SYMBOL(set_enqueue_hook);
+
 static void enqueue_huge_page(struct hstate *h, struct page *page)
 {
 	int nid = page_to_nid(page);
 
+	if (enqueue_hook) {
+		enqueue_hook(h, page);
+		return;
+	}
+
 	lockdep_assert_held(&hugetlb_lock);
 	VM_BUG_ON_PAGE(page_count(page), page);
 
@@ -1288,11 +1303,21 @@ static void enqueue_huge_page(struct hstate *h, struct page *page)
 	SetHPageFreed(page);
 }
 
+static dequeue_hook_t dequeue_hook = NULL;
+void set_dequeue_hook(dequeue_hook_t hook)
+{
+	dequeue_hook = hook;
+}
+EXPORT_SYMBOL(set_dequeue_hook);
+
 static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)
 {
 	struct page *page;
 	bool pin = !!(current->flags & PF_MEMALLOC_PIN);
 
+	if (dequeue_hook)
+		return dequeue_hook(h, nid);
+
 	lockdep_assert_held(&hugetlb_lock);
 	list_for_each_entry(page, &h->hugepage_freelists[nid], lru) {
 		if (pin && !is_longterm_pinnable_page(page))
@@ -1871,6 +1896,7 @@ struct hstate *size_to_hstate(unsigned long size)
 	}
 	return NULL;
 }
+EXPORT_SYMBOL(size_to_hstate);
 
 void free_huge_page(struct page *page)
 {
@@ -7369,6 +7395,7 @@ int isolate_hugetlb(struct page *page, struct list_head *list)
 	spin_unlock_irq(&hugetlb_lock);
 	return ret;
 }
+EXPORT_SYMBOL(isolate_hugetlb);
 
 int get_hwpoison_huge_page(struct page *page, bool *hugetlb)
 {
@@ -7407,6 +7434,7 @@ void putback_active_hugepage(struct page *page)
 	spin_unlock_irq(&hugetlb_lock);
 	put_page(page);
 }
+EXPORT_SYMBOL(putback_active_hugepage);
 
 void move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason)
 {
diff --git a/mm/migrate.c b/mm/migrate.c
index 8d5c0dc61..a6d7b96a3 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -937,9 +937,10 @@ static int move_to_new_folio(struct folio *dst, struct folio *src,
 	if (likely(is_lru)) {
 		struct address_space *mapping = folio_mapping(src);
 
-		if (!mapping)
+		if (!mapping) {
 			rc = migrate_folio(mapping, dst, src, mode);
-		else if (mapping->a_ops->migrate_folio)
+		}
+		else if (mapping->a_ops->migrate_folio) {
 			/*
 			 * Most folios have a mapping and most filesystems
 			 * provide a migrate_folio callback. Anonymous folios
@@ -949,8 +950,10 @@ static int move_to_new_folio(struct folio *dst, struct folio *src,
 			 */
 			rc = mapping->a_ops->migrate_folio(mapping, dst, src,
 								mode);
-		else
+		}
+		else {
 			rc = fallback_migrate_folio(mapping, dst, src, mode);
+		}
 	} else {
 		const struct movable_operations *mops;
 
@@ -1308,8 +1311,9 @@ static int unmap_and_move_huge_page(new_page_t get_new_page,
 	if (folio_test_anon(src))
 		anon_vma = folio_get_anon_vma(src);
 
-	if (unlikely(!folio_trylock(dst)))
+	if (unlikely(!folio_trylock(dst))) {
 		goto put_anon;
+	}
 
 	if (folio_mapped(src)) {
 		enum ttu_flags ttu = 0;
@@ -1322,8 +1326,9 @@ static int unmap_and_move_huge_page(new_page_t get_new_page,
 			 * to let lower levels know we have taken the lock.
 			 */
 			mapping = hugetlb_page_mapping_lock_write(hpage);
-			if (unlikely(!mapping))
+			if (unlikely(!mapping)) {
 				goto unlock_put_anon;
+			}
 
 			ttu = TTU_RMAP_LOCKED;
 		}
@@ -1604,6 +1609,7 @@ int migrate_pages(struct list_head *from, new_page_t get_new_page,
 
 	return rc;
 }
+EXPORT_SYMBOL(migrate_pages);
 
 struct page *alloc_migration_target(struct page *page, unsigned long private)
 {
diff --git a/mm/vmscan.c b/mm/vmscan.c
index d18296109..815043271 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -1440,6 +1440,18 @@ void folio_putback_lru(struct folio *folio)
 	folio_put(folio);		/* drop ref from isolate */
 }
 
+void mtat_folio_putback_lru(struct folio *folio)
+{
+	folio_putback_lru(folio);
+}
+EXPORT_SYMBOL(mtat_folio_putback_lru);
+
+int mtat_folio_isolate_lru(struct folio *folio)
+{
+	return folio_isolate_lru(folio);
+}
+EXPORT_SYMBOL(mtat_folio_isolate_lru);
+
 enum folio_references {
 	FOLIOREF_RECLAIM,
 	FOLIOREF_RECLAIM_CLEAN,
